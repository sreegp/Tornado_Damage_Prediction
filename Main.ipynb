{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import data_collection_and_merging, data_preprocessing\n",
    "from scaling_and_sampling import scaling_train_test_split, sampling_and_baseline_model\n",
    "from model_selection import models_and_hyperparameter_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collection and Merging\n",
    "storm_events_file = 'StormEvents_details-ftp_v1.0_d'\n",
    "income_data_file = 'Median_Household_Income.csv'\n",
    "population_data_file = 'Population_Density.csv'\n",
    "data_collection = data_collection_and_merging\n",
    "tornado_data = data_collection.get_tornado_data(storm_events_file)\n",
    "income_data = data_collection.get_income_data(income_data_file)\n",
    "population_data = data_collection.get_population_data(population_data_file)\n",
    "merged_data = data_collection.merge_data(tornado_data, income_data, population_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "data_processing = data_preprocessing(merged_data)\n",
    "data_processing.day_of_year_week_weekend()\n",
    "data_processing.drop_and_rename_columns()\n",
    "data_processing.create_duration()\n",
    "data_processing.create_casualties_column()\n",
    "data_processing.calc_tornado_area()\n",
    "data_processing.calc_min_and_avg_range()\n",
    "data_processing.calc_avg_lat_and_long()\n",
    "data_processing.calc_percentage_land()\n",
    "data_processing.extract_multi_vortex_ref()\n",
    "data_processing.fillna()\n",
    "data_processing.sin_and_cosine_time()\n",
    "data_processing.binary_tornado_intensity_estimate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the 93rd percentile of casualties is 1, we'll binarize casualties to predict whether or not tornadoes will have casualties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14557.000000\n",
       "mean         0.941952\n",
       "std         16.158968\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "90%          0.000000\n",
       "93%          1.000000\n",
       "95%          2.000000\n",
       "max       1311.000000\n",
       "Name: casualties, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processing.data['casualties'].describe(percentiles = [.25, .5, .75, .9, .93,.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processing.binarize_casualties()\n",
    "processed_data = data_processing.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_and_train_test_split = scaling_train_test_split\n",
    "train, val, test = scaling_and_train_test_split.train_and_test_split(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for percentage of class 1 samples in training, validation and test sets. Since the percentage is fairly similar, we don't have to stratify the data further and can go ahead and scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08321167883211679"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train['binary_casualties'][train['binary_casualties'] == 1])/len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.053595968850206135"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val['binary_casualties'][val['binary_casualties'] == 1])/len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07402618393405527"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test['binary_casualties'][test['binary_casualties'] == 1])/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_trainX, trainY, scaled_valX, valY, scaled_testX, testY = scaling_and_train_test_split.scaling(train, val, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling = sampling_and_baseline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7658115521135851\n",
      "[[1787  279]\n",
      " [  39   78]]\n"
     ]
    }
   ],
   "source": [
    "sampling.random_oversampling(scaled_trainX, trainY, scaled_valX, valY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7571052696899744\n",
      "[[1804  262]\n",
      " [  42   75]]\n"
     ]
    }
   ],
   "source": [
    "sampling.SMOTE_oversampling(scaled_trainX, trainY, scaled_valX, valY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7549912709641655\n",
      "[[1654  412]\n",
      " [  34   83]]\n"
     ]
    }
   ],
   "source": [
    "sampling.ADASYN_oversampling(scaled_trainX, trainY, scaled_valX, valY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7529848338173604\n",
      "[[1734  332]\n",
      " [  39   78]]\n"
     ]
    }
   ],
   "source": [
    "sampling.random_undersampling(scaled_trainX, trainY, scaled_valX, valY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.739138349012502\n",
      "[[1924  142]\n",
      " [  53   64]]\n"
     ]
    }
   ],
   "source": [
    "sampling.Edited_NN_undersampling(scaled_trainX, trainY, scaled_valX, valY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Random Oversampling as it's performs the best and undersampling might reduce the size of the dataset (12K rows) even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = models_and_hyperparameter_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:06,  1.19s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_values</th>\n",
       "      <th>Penalty</th>\n",
       "      <th>Mean_AUC</th>\n",
       "      <th>Standard_Deviation_AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.780239</td>\n",
       "      <td>0.032806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   C_values Penalty  Mean_AUC  Standard_Deviation_AUC\n",
       "2     0.001      l1  0.780239                0.032806"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_values = [0.0001, 0.001,0.01,0.1,1,10,100,1000]\n",
    "penalty = ['l1','l2']\n",
    "models.Logistic_Regression(scaled_trainX, trainY, C_values, penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:55, 11.00s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Min_Samples_Split</th>\n",
       "      <th>Max_Depth</th>\n",
       "      <th>Mean_AUC</th>\n",
       "      <th>Standard_Deviation_AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.778499</td>\n",
       "      <td>0.031487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.778499</td>\n",
       "      <td>0.031487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>82</td>\n",
       "      <td>2</td>\n",
       "      <td>0.778499</td>\n",
       "      <td>0.031487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>122</td>\n",
       "      <td>2</td>\n",
       "      <td>0.778499</td>\n",
       "      <td>0.031487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>162</td>\n",
       "      <td>2</td>\n",
       "      <td>0.778499</td>\n",
       "      <td>0.031487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>202</td>\n",
       "      <td>2</td>\n",
       "      <td>0.778499</td>\n",
       "      <td>0.031487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>242</td>\n",
       "      <td>2</td>\n",
       "      <td>0.778499</td>\n",
       "      <td>0.031487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>282</td>\n",
       "      <td>2</td>\n",
       "      <td>0.778499</td>\n",
       "      <td>0.031487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>322</td>\n",
       "      <td>2</td>\n",
       "      <td>0.778499</td>\n",
       "      <td>0.031487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>362</td>\n",
       "      <td>2</td>\n",
       "      <td>0.778499</td>\n",
       "      <td>0.031487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Min_Samples_Split  Max_Depth  Mean_AUC  Standard_Deviation_AUC\n",
       "0                   2          2  0.778499                0.031487\n",
       "10                 42          2  0.778499                0.031487\n",
       "20                 82          2  0.778499                0.031487\n",
       "30                122          2  0.778499                0.031487\n",
       "40                162          2  0.778499                0.031487\n",
       "50                202          2  0.778499                0.031487\n",
       "60                242          2  0.778499                0.031487\n",
       "70                282          2  0.778499                0.031487\n",
       "80                322          2  0.778499                0.031487\n",
       "90                362          2  0.778499                0.031487"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_samples_split_values = list(range(2,400,40))\n",
    "maxdepth = list(range(2,400,40))\n",
    "models.Decision_Trees(scaled_trainX, trainY, min_samples_split_values, maxdepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [01:24, 28.14s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No_of_Estimators</th>\n",
       "      <th>Max_Depth</th>\n",
       "      <th>Min_Split</th>\n",
       "      <th>Mean_AUC</th>\n",
       "      <th>Standard_Deviation_AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.786436</td>\n",
       "      <td>0.033457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    No_of_Estimators  Max_Depth  Min_Split  Mean_AUC  Standard_Deviation_AUC\n",
       "20                10          5          2  0.786436                0.033457"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [1,10,50,100]\n",
    "max_depth = [2,5,10,20]\n",
    "min_split = [2,42,162,362]\n",
    "models.Random_Forest(scaled_trainX, trainY,estimators, max_depth, min_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression models performs the best and will be evaluated on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
